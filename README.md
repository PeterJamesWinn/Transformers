# Transformers
Coding a generative self attention transformer follow Andrej Karpathy's YouTube tutorial: https://www.youtube.com/watch?v=kCc8FmEb1nY&t=5568s
ChatGPT_type_transformer_initial_bigram_playing.ipynb  - playing around with initial code: Bigram model base - attention not yet added"
ChatGPT_type_model_tidied.ipynb - a tidied version of *_initial_bigram_playing.ipynb, with a basic transofrmer block added - self attention, mlp and residual/skip connections. No dropout (yet). Not close to proudction code standard, but allows playing with the basic workings of the transformer block. 
