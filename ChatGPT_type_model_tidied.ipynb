{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1U-CzyEEpfQ"
      },
      "outputs": [],
      "source": [
        "# from Andrej Karpathy's tutorial on ChatGPT - a tidied up notebook of the initial code"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8OlY7xNE5V_",
        "outputId": "fa83a02c-5b7c-4374-f749-dc12e9ed4e50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-03 18:32:47--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.009s  \n",
            "\n",
            "2023-06-03 18:32:47 (115 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "batch_size = 32 # how many independent seqeunces will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions? \n",
        "n_embed = 32\n",
        "\n",
        "training_iterations = 5000\n",
        "evaluation_interval = 500 #validation every evaluation_interval steps of optimsation.\n",
        "# take average of evaluation_batches batches for loss calculations \n",
        "#  in the function estimate_loss, to reduce the noise in the estimates.\n",
        "evaluation_batches = 20 \n",
        "learning_rate = 1e-3\n",
        "# device agnostic definition\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def get_batch(split): # data loader\n",
        "  # generate a small batch of data of inputs x and targets y\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  #randomly pick starts for batch_size number of smaples and then loop through\n",
        "  # batch samples, stacking each sample in a row to give a batch * block_size tensor\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,)) \n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1: i+block_size+1] for i in ix])\n",
        "  x.to(device), y.to(device)\n",
        "  return x, y \n",
        "\n",
        "# below function new code compared to what was in the Jupyter notebook\n",
        "# used for sketching out details. \n",
        "@torch.no_grad()\n",
        "def estimate_loss(eval_batches):\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in ['train', 'val']:\n",
        "    losses = torch.zeros(eval_batches)\n",
        "    for k in range(eval_batches):\n",
        "      X, Y = get_batch(split)\n",
        "      logits, loss = model(X, Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \n",
        "    def __init__(self, n_embed):\n",
        "      super().__init__()\n",
        "      self.ff = nn.Sequential(\n",
        "          nn.Linear(n_embed, 4*n_embed),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(4*n_embed, n_embed)\n",
        "      )\n",
        "\n",
        "    def forward(self, x):\n",
        "      return self.ff(x)\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "  \"\"\"one head of self-attention\"\"\"\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embed, head_size, bias = False) # (B, T, 16)\n",
        "    self.query = nn.Linear(n_embed, head_size, bias=False) # (B, T, 16)\n",
        "    self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "    # lower triangular matrix, below is not needed for encoder, but decoder needs\n",
        "    # past not to see the future, so needs masking.\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "  def forward(self,x):\n",
        "    B, T, C = x.shape # batch, time, channels\n",
        "    # Each token learns during training to give vector that is a query\n",
        "    # and one that is a key. \n",
        "    # These represent \n",
        "    # Query - what looking for? What in my local environment is interesting? \n",
        "    # Key - what do I contain? What do I have that another token might need?\n",
        "    # query dot key indicates how well these two match.\n",
        "    # value is the learnt value that should be emitted for a match.\n",
        "    k = self.key(x)\n",
        "    q = self.query(x)\n",
        "    v = self.value(x)\n",
        "    weights = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, 16) @ (B, 16, T) --> (B, T, T)\n",
        "    weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # not needed for encoder\n",
        "    weights = F.softmax(weights, dim=-1)\n",
        "    out = weights @ v \n",
        "    return out\n",
        "# attention is a communication mechanism. Take the information and transform it.\n",
        "# no notion of space therefore need to code positional information. \n",
        "# batches are independent of each other.\n",
        "# this is a decoder so past cannot see the future. Encoder head does not have\n",
        "# this restriction and removes the lower triangular matrix filter\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  '''multiple heads of self-attention in parallel'''\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    self.proj = nn.Linear(n_embed, n_embed) \n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    return self.proj(out)\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "  ''' pass input through multiheaded attention and then feed forward mlp. '''\n",
        "  # transformer block - communication followed by multilayer perceptron \n",
        "  # communication.\n",
        "  def __init__(self, n_embed, n_head):\n",
        "    super().__init__()\n",
        "    head_size = n_embed // n_head\n",
        "    self.layer_norm1 = nn.LayerNorm(n_embed)\n",
        "    self.sa = MultiHeadAttention(n_head, head_size)\n",
        "    self.layer_norm2 = nn.LayerNorm(n_embed)\n",
        "    self.fforward = FeedForward(n_embed)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.layer_norm1(x)\n",
        "    x = x + self.sa(x)\n",
        "    x = self.layer_norm2(x)\n",
        "    x = x + self.fforward(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    # basic bigram read each token directly reads off the logits for the next \n",
        "    # token from a lookup table, so could be interpretted as a probability via \n",
        "    # a softmax layer, \n",
        "    # but now the embedings will not be the probability of the next character,\n",
        "    # being instead a latent representation that will get processed through \n",
        "    # further layers before producing the logits that feed into a softmax.\n",
        "    # The embedding table is thus now of size vocab_size * n_embed\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "    #self.sa_heads = MultiHeadAttention(4, n_embed//4)\n",
        "    #self.fforward = FeedForward(n_embed)\n",
        "    # replace above two entries with a transformer block containing both\n",
        "    self.transformer_blocks = nn.Sequential(\n",
        "        TransformerBlock(n_embed, n_head=4),\n",
        "        TransformerBlock(n_embed, n_head=4),\n",
        "        TransformerBlock(n_embed, n_head=4),\n",
        "        nn.LayerNorm(n_embed)\n",
        "    )\n",
        "    self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B, T = idx.shape\n",
        "    # nn module of pytorch, which this inherits from runs forward when called\n",
        "    # e.g. below as \n",
        "    # m = BigramLanguageModel(vocab_size)\n",
        "    # logits, loss = m(xb, yb)\n",
        "    # idx - (index of) the x values, i.e. the context vector\n",
        "    # idx and targets are both (B, T) tensor of integers (see comment below)\n",
        "    tok_emb = self.token_embedding_table(idx) # (B, T, C)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "    x = tok_emb + pos_emb # (B, T, C)\n",
        "    #x = self.sa_heads(x1)\n",
        "    #x = self.fforward(x)\n",
        "    #x = x + x1\n",
        "    # replace above two lines with transformer blocks, defined at instantiation\n",
        "    x = self.transformer_blocks(x)\n",
        "    logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "    # B T C is batch by time by channel\n",
        "    # batch  is 4\n",
        "    # time is 8 (block size)\n",
        "    # channel is 65 i.e. vocab size i.e. which element of vocab\n",
        "    # with row of embedding table representing likely next character of the sequence.\n",
        "    # Summary: The embedding table receives a tensor, idx, of dimension B*T and \n",
        "    # for each entry returns a dimension of size C.\n",
        "    if targets == None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape # need to reshape to be correct shape for cross_entropy\n",
        "      # use view in pytorch that changes the view of the data passed but not the\n",
        "      # underlying data. Flatten the first two dimensions, leaving the channel (vocab)\n",
        "      # as second dimension, which is what F.cross_entropy expects.\n",
        "      # print(f\"Targets {targets} \\n; logits {logits} \")\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "    return logits, loss\n",
        "  \n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    '''Predict the next character/token from the learnt distribution,adding\n",
        "    it to the current context, idx, till\n",
        "    max_new_tokens have been added.'''\n",
        "    for _ in range(max_new_tokens):\n",
        "      #get the predictions\n",
        "      # crop idx to the last block_size number of tokens, which is needed\n",
        "      # since the attention layer has positional embedding only up to length\n",
        "      # block size\n",
        "      idx_cond = idx[:,-block_size:]\n",
        "      logits, loss = self(idx_cond) # calls forwards() for this class\n",
        "      # next line gets the last time step, i.e. the last character for \n",
        "      # each sequence in the\n",
        "      # batch, taking all batches and all embeddings for that character, and place\n",
        "      # in logits.\n",
        "      logits = logits[:, -1, :] # when we develop this later we'll focus on more \n",
        "      # context but currently only have bigram model, so just taking last \n",
        "      # token/logit.\n",
        "      # so logits is now dimension B, C, since only one token\n",
        "      # generate probability distribution\n",
        "      probs = F.softmax(logits, dim=-1) # B, C\n",
        "      # sample from the vocab according to the probability distribution encoded\n",
        "      # by the logits\n",
        "      idx_next = torch.multinomial(probs, num_samples = 1) # B by 1 size\n",
        "      # append sample to sequence\n",
        "      idx = torch.cat((idx, idx_next), dim = 1)\n",
        "    return idx\n",
        "\n",
        "text = open('input.txt','r',encoding='utf-8').read()\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# create a mapping from characters to integers and reverse\n",
        "stoi = { ch:i for i, ch in enumerate(chars)}  #dictionary with key character, token i\n",
        "itos = { i:ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s] #encode string as integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) #decode list of integers as output string\n",
        "data = torch.tensor(encode(text), dtype = torch.long)\n",
        "\n",
        "#simple train /validation sets. No test set. no randomisation of selections, so assuming\n",
        "# no bias in the distribution within the data file!\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size + 1] # we're training to predict the next character.\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "m = model.to(device)\n",
        "logits, loss = m(xb, yb)\n",
        "print(f\"untrained loss: {loss}\")\n",
        "\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(\"Text generated from untrained network: \")\n",
        "print(decode(m.generate(context, max_new_tokens = 100)[0].tolist()))\n",
        "\n",
        "# training loop. \n",
        "# high learning rate since simple network\n",
        "print(f\"training for {training_iterations} iterations\")\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate) \n",
        "\n",
        "for iter in range(training_iterations):\n",
        "  xb, yb = get_batch('train')\n",
        "  #xval, yval = get_batch('validation')\n",
        "\n",
        "  logits, loss = m(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  #equally uncertain distribution implies 1/vocab_size = 1/65 probability of each \n",
        "  # next character which implies -ln(1/65) = 4.17 would be optimal initial loss\n",
        "  if iter % evaluation_interval == 0:\n",
        "    #logits_val, loss_val = m(xval, yval)\n",
        "    #print(f\"training loss: {loss.item()}, \\\n",
        "    #                                       validation loss {loss_val.item()}\")\n",
        "    losses = estimate_loss(evaluation_batches)\n",
        "    print(f\"step {iter}: train loss {losses['train']:.4f}, \\\n",
        "                                         validation loss {losses['val']:.4f})\")\n",
        "print(f\"Text from trained network, trained {training_iterations} iterations:\")\n",
        "print(decode(m.generate(context, max_new_tokens = 100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jR2Xc-vE9_Z",
        "outputId": "f4ec72bb-f23e-4c31-a50f-3c57213ed465"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "untrained loss: 4.33352518081665\n",
            "Text generated from untrained network: \n",
            "\n",
            "FHcJksEx\n",
            "VRHA3Z?CLY Xkg\n",
            "FAr;G'z-DY,Gr&j;IU&xXLMWl,Y?GMclS-JLjlKkTXT,sNQNq&xYd$bMftRGSLDiYXCmJIbPBMPt\n",
            "training for 5000 iterations\n",
            "step 0: train loss 4.2323,                                          validation loss 4.2459)\n",
            "step 500: train loss 2.4130,                                          validation loss 2.4179)\n",
            "step 1000: train loss 2.2572,                                          validation loss 2.2725)\n",
            "step 1500: train loss 2.1664,                                          validation loss 2.1889)\n",
            "step 2000: train loss 2.0823,                                          validation loss 2.1343)\n",
            "step 2500: train loss 2.0130,                                          validation loss 2.0842)\n",
            "step 3000: train loss 1.9974,                                          validation loss 2.0700)\n",
            "step 3500: train loss 1.9662,                                          validation loss 2.0457)\n",
            "step 4000: train loss 1.9458,                                          validation loss 2.0264)\n",
            "step 4500: train loss 1.9016,                                          validation loss 1.9846)\n",
            "Text from trained network, trained 5000 iterations:\n",
            "\n",
            "SICHARD IIII:\n",
            "Be this boy knowes, stome,\n",
            "By I mother with with his. Wory,\n",
            "Shou bloin huven in now yo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below just playing with batch norm and layer norm. Not used in the above code. "
      ],
      "metadata": {
        "id": "veZafMrSgy7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BatchNorm:\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.momentum = momentum\n",
        "    self.training = True\n",
        "    #parameters trained with backprop)\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = toch.zeros(dim)\n",
        "    #buffers (trained with a running 'momentum update')\n",
        "    self.running_mean = torch.zeros(dim)\n",
        "    self.running_var = torch.ones(dim)\n",
        "\n",
        "    def __call__(self, x):\n",
        "      #calculate forward pass\n",
        "      if self.training: \n",
        "        xmean = x.mean(0, keepdim=True) # batch mean\n",
        "        xvar = x.var(0, keepdim=True) # batch variance\n",
        "      else:\n",
        "        xmean = self.running_mean\n",
        "        xvar = self.running_var\n",
        "      xhat = (x - xmean) /toch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "      self.out = self.gamma * xhat + self.beta\n",
        "      # update the buffers\n",
        "      if self.training:\n",
        "        with torch.no_grad():\n",
        "          self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
        "          self.running_var = (1 - self.momentum) * self.running_var + self.momentum * var\n",
        "      return self.out\n",
        "\n",
        "    def parameters(self):\n",
        "      return [self.gamma, self.beta]"
      ],
      "metadata": {
        "id": "Bp3BEHbSZw90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm:\n",
        "  def __init__(self, dim, eps=1e-5):\n",
        "    self.eps = eps\n",
        "    #parameters trained with backprop)\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "\n",
        "    def __call__(self, x):\n",
        "      #calculate forward pass\n",
        "      xmean = x.mean(1, keepdim=True) #  mean of layer\n",
        "      xvar = x.var(1, keepdim=True) #  variance of layer\n",
        "      xhat = (x - xmean) /toch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "      self.out = self.gamma * xhat + self.beta\n",
        "      return self.out\n",
        "\n",
        "    def parameters(self):\n",
        "      return [self.gamma, self.beta]"
      ],
      "metadata": {
        "id": "AeUzSihXc8yW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "#a = torch.randn(4,4)\n",
        "a = torch.ones(4, 3)\n",
        "a\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFJumHiYdxdg",
        "outputId": "3c695833-71d0-491b-83d0-d5f1f72ff81d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a[1,1]=3\n",
        "a.mean(1, keepdim=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkJJzZnbd5df",
        "outputId": "4ff0587c-e334-46bf-b434-30233bf9812a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000],\n",
              "        [1.6667],\n",
              "        [1.0000],\n",
              "        [1.0000]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a.mean(0, keepdim=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2EELCwZe1Ny",
        "outputId": "984b5664-2bae-4e44-9b94-b2aa5ae3013f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 1.5000, 1.0000]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}